{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 align=\"right\">by <a href=\"http://cse.iitkgp.ac.in/~adas/\">Abir Das</a> with help of <br> Ram Rakesh and Ankit Singh<br> </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the following details here\n",
    "** Name: ** BHUVAN MERAWI<br/>\n",
    "** Roll Number: ** 15IM30004<br/>\n",
    "** Department: ** INDUSTRIAL AND SYSTEMS ENGINEERING<br/>\n",
    "** Email: ** bhuvanmerawi@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "To run and solve this assignment, one must have a working IPython Notebook installation. The easiest way to set it up for both Windows and Linux is to install [Anaconda](https://www.continuum.io/downloads). Then save this file ([`assignment_01.ipynb`]()) to your computer, run Anaconda and choose this file in Anaconda's file explorer. Use `Python 3` version. Below statements assume that you have already followed these instructions. If you are new to Python or its scientific library, Numpy, there are some nice tutorials [here](https://www.learnpython.org/) and [here](http://www.scipy-lectures.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem: You will implement a fully connected neural network from scratch in this problem\n",
    "We marked places where you are expected to add/change your own code with **`##### write your code below #####`** comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "597wDiAvGvuB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are not supposed to import any other python library to work on this assignments.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "'''You are not supposed to import any other python library to work on this assignments.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "B54oZmm1DNWe",
    "outputId": "8c59bd48-230d-4fb9-eba1-82471de363df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n"
     ]
    }
   ],
   "source": [
    "'''data is loaded from data directory.\n",
    "please don't remove the folder '''\n",
    "\n",
    "x_train = np.load('./data/X_train.npy')\n",
    "x_train = x_train.flatten().reshape(-1,28*28)\n",
    "x_train = x_train / 255.0\n",
    "gt_indices = np.load('./data/y_train.npy')\n",
    "train_length = len(x_train)\n",
    "print(\"Number of training examples: {:d}\".format(train_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LvVFhXNB5xrD"
   },
   "outputs": [],
   "source": [
    "'''Dimensions to be used for creating your model'''\n",
    "\n",
    "batch_size = 64  # batch size\n",
    "input_dim = 784  # input dimension\n",
    "hidden_1_dim = 512  # hidden layer 1 dimension\n",
    "hidden_2_dim = 256  # hidden layer 2 dimension\n",
    "output_dim = 10   # output dimension\n",
    "\n",
    "'''Other hyperparameters'''\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hImaaujc5zXg"
   },
   "outputs": [],
   "source": [
    "#creating one hot vector representation of output classification\n",
    "y_train = np.zeros((train_length, output_dim))\n",
    "#print(y_train.shape, gt_indices.shape)\n",
    "for i in range(train_length):\n",
    "    y_train[i,gt_indices[i]] = 1\n",
    "\n",
    "# Number of mini-batches (as integer) in one epoch\n",
    "num_minibatches = np.floor(train_length/batch_size).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W7lHWEWVaVlK",
    "outputId": "4ecb1bfc-4568-44cb-e109-57677da50eb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of mini-batches 937 and total training data used in training:59968.\n"
     ]
    }
   ],
   "source": [
    "print(\"No of mini-batches {:d} and total training data used in training:\\\n",
    "{}.\".format(num_minibatches, num_minibatches*batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9HRf0Wj52cK"
   },
   "outputs": [],
   "source": [
    "'''Randomly Initialize Weights  from standard normal distribution (i.e., mean = 0 and s.d. = 1.0).\n",
    "Use the dimesnions specified in the cell 3 to initialize your weights matrices. \n",
    "Use the nomenclature W1,W2 etc. (provided below) for the different weight matrices.'''\n",
    "\n",
    "########################## write your code below ##############################################\n",
    "W1 = np.random.normal(0.0, 1.0, 784*512).reshape(784,512)\n",
    "W2 = np.random.normal(0.0, 1.0, 512*256).reshape(512,256)\n",
    "W3 = np.random.normal(0.0, 1.0, 256*10).reshape(256,10)\n",
    "\n",
    "###############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PmZRrEVb6CJy"
   },
   "outputs": [],
   "source": [
    "# Write a function which computes the softmax where X is vector of scores computed during forward pass\n",
    "def softmax(x):\n",
    "    ##############################write your code here #################################\n",
    "#    return (np.exp(x)/np.sum(np.exp(x),axis=1, keepdims=True))\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps/np.sum(exps, axis=1, keepdims=True)\n",
    "    ####################################################################################\n",
    "    pass\n",
    "\n",
    "#def relu(x):\n",
    "#    r=x.shape[0]\n",
    "#    c=x.shape[1]\n",
    "#    for i in range(r):\n",
    "#        for j in range(c):\n",
    "#            x[i][j]=max(0,x[i][j])\n",
    "#    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "Gjz4yhwE6JQw",
    "outputId": "341578db-29a4-48ca-b0f8-a0343aadd24b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 0, iteration: 0, Loss: 17.5571 \n",
      " Epoch: 1, iteration: 937, Loss: 2.8782 \n",
      " Epoch: 2, iteration: 1874, Loss: 2.8782 \n",
      " Epoch: 3, iteration: 2811, Loss: 2.5904 \n",
      " Epoch: 4, iteration: 3748, Loss: 2.3026 \n",
      " Epoch: 5, iteration: 4685, Loss: 2.0148 \n",
      " Epoch: 6, iteration: 5622, Loss: 2.0148 \n",
      " Epoch: 7, iteration: 6559, Loss: 2.0147 \n",
      " Epoch: 8, iteration: 7496, Loss: 1.7269 \n",
      " Epoch: 9, iteration: 8433, Loss: 1.7269 \n",
      " Epoch: 10, iteration: 9370, Loss: 1.7269 \n",
      " Epoch: 11, iteration: 10307, Loss: 1.7269 \n",
      " Epoch: 12, iteration: 11244, Loss: 1.7269 \n",
      " Epoch: 13, iteration: 12181, Loss: 1.6359 \n",
      " Epoch: 14, iteration: 13118, Loss: 1.4391 \n",
      " Epoch: 15, iteration: 14055, Loss: 1.4391 \n",
      " Epoch: 16, iteration: 14992, Loss: 1.4391 \n",
      " Epoch: 17, iteration: 15929, Loss: 1.4391 \n",
      " Epoch: 18, iteration: 16866, Loss: 1.4391 \n",
      " Epoch: 19, iteration: 17803, Loss: 1.4391 \n",
      " Epoch: 20, iteration: 18740, Loss: 1.4109 \n",
      " Epoch: 21, iteration: 19677, Loss: 1.3684 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "no_of_iterations = 20000\n",
    "loss_list=[]\n",
    "i_epoch = 0\n",
    "for i_iter in range(no_of_iterations):\n",
    "    \n",
    "    ''''''\n",
    "    batch_elem_idx = i_iter%num_minibatches\n",
    "    x_batchinput = x_train[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size]\n",
    "    \n",
    "    ########################## write your code below ##############################################\n",
    "    ######################### Forward Pass Block #####################################\n",
    "    '''Write the code for forward block of the neural network with 2 hidden layers.\n",
    "    Please stick to the notation below which follows the notation provided in the lecture slides.\n",
    "    Note that you are allowed to write the right hand sides of these variables in more than\n",
    "    one line if that is convenient for you.'''\n",
    "    \n",
    "    # first hidden layer implementation\n",
    "    a1 =np.dot(x_batchinput,W1) \n",
    "    # implement Relu layer\n",
    "    h1 = np.maximum(0, a1)\n",
    "    #  implement 2 hidden layer\n",
    "    a2 = np.dot(h1,W2)\n",
    "    # implement Relu activation \n",
    "    h2 = np.maximum(0, a2)\n",
    "    #implement linear output layer\n",
    "    a3 = np.dot(h2,W3)\n",
    "    # softmax layer\n",
    "    softmax_score = softmax(a3) #enusre you have implemented the softmax function defined above\n",
    "    ##################################################################################\n",
    "    ###############################################################################################\n",
    "\n",
    "    neg_log_softmax_score = -np.log(softmax_score+0.00000001) # The small number is added to avoid 0 input to log function\n",
    "    \n",
    "    # Compute and print loss\n",
    "    if i_iter%num_minibatches == 0:\n",
    "        loss = np.mean(np.diag(np.take(neg_log_softmax_score, gt_indices[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size],\\\n",
    "                                       axis=1)))\n",
    "        print(\" Epoch: {:d}, iteration: {:d}, Loss: {:6.4f} \".format(i_epoch, i_iter, loss))\n",
    "        loss_list.append(loss)\n",
    "        i_epoch += 1\n",
    "        # Each 10th epoch reduce learning rate by a factor of 10\n",
    "        if i_epoch%10 == 0:\n",
    "            learning_rate /= 10.0\n",
    "     \n",
    "    ################################### Backpropagation Code Block #####################################\n",
    "    ########################## write your code below ##############################################\n",
    "    # Gradient of cross-entropy loss w.r.t. preactivation of the output layer\n",
    "    grad_softmax_score =-(y_train[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size] - softmax_score)\n",
    "    \n",
    "    # gradient w.r.t W3\n",
    "    grad_W3 = np.dot(np.transpose(h2), grad_softmax_score)\n",
    "    # gradient w.r.t h2\n",
    "    grad_h2 =  np.dot(grad_softmax_score, np.transpose(W3))\n",
    "    # gradient w.r.t a2\n",
    "    grad_a2 = np.zeros(a2.shape)\n",
    "    grad_a2[a2>0] = 1\n",
    "    # gradient w.r.t W2\n",
    "    grad_W2 = np.dot(np.transpose(h1), grad_a2*grad_h2)\n",
    "    # gradient w.r.t h1\n",
    "    grad_h1 = np.dot(grad_softmax_score, np.dot(np.transpose(W3), np.transpose(W2)))\n",
    "    # gradient w.r.t a1\n",
    "    grad_a1 = np.zeros(a1.shape)\n",
    "    grad_a1[a1>0] = 1 \n",
    "    # gradient w.r.t W1\n",
    "    grad_W1 = np.dot(np.transpose(x_batchinput), grad_a1*grad_h1)\n",
    "    ###############################################################################################\n",
    "    ####################################################################################################\n",
    "    \n",
    "    ################################ Update Weights Block using SGD ####################################\n",
    "    W3 -= learning_rate * grad_W3\n",
    "    W2 -= learning_rate * grad_W2\n",
    "    W1 -= learning_rate * grad_W1\n",
    "    ####################################################################################################\n",
    "    \n",
    "#plotting the loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(loss_list)\n",
    "plt.title('Loss vs epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loading the test data from data/X_test.npy and data/y_test.npy.'''\n",
    "x_test = np.load('./data/X_test.npy')\n",
    "x_test = x_test.flatten().reshape(-1,28*28)\n",
    "x_test = x_test / 255.0\n",
    "y_test = np.load('./data/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 92.00 %\n"
     ]
    }
   ],
   "source": [
    "batch_size_test = 100 # Deliberately taken 100 so that it divides the test data size\n",
    "num_minibatches = len(y_test)/batch_size_test\n",
    "test_correct = 0\n",
    "\n",
    "'''Only forward block code and compute softmax_score .'''\n",
    "for i_iter in range(int(num_minibatches)):\n",
    "    \n",
    "    '''Get one minibatch'''\n",
    "    batch_elem_idx = i_iter%num_minibatches\n",
    "    x_batchinput = x_test[i_iter*batch_size_test:(i_iter+1)*batch_size_test]\n",
    "    \n",
    "    ######### copy only the forward pass block of your code and pass the x_batchinput to it and compute softmax_score ##########\n",
    "    # first hidden layer implementation\n",
    "    a1 =np.dot(x_batchinput,W1) \n",
    "    # implement Relu layer\n",
    "    h1 = np.maximum(0, a1)\n",
    "    #  implement 2 hidden layer\n",
    "    a2 = np.dot(h1,W2)\n",
    "    # implement Relu activation \n",
    "    h2 = np.maximum(0, a2)\n",
    "    #implement linear output layer\n",
    "    a3 = np.dot(h2,W3)\n",
    "    # softmax layer\n",
    "    softmax_score = softmax(a3) #enusre you have implemented the softmax function defined above\n",
    "    ##################################################################################\n",
    "    \n",
    "    y_batchinput = y_test[i_iter*batch_size_test:(i_iter+1)*batch_size_test]\n",
    "    \n",
    "    y_pred = np.argmax(softmax_score, axis=1)\n",
    "    num_correct_i_iter = np.sum(y_pred == y_batchinput)\n",
    "    test_correct += num_correct_i_iter\n",
    "print (\"Test accuracy is {:4.2f} %\".format(test_correct/len(y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2_Hidden_MLP_New.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
